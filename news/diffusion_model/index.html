<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Diffusion Model - A Math-based Simplified Understanding | Umi's Corner </title> <meta name="author" content="Umi's Corner"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="machine learning, deep learning, diffusion model, generative models, LLM, computer vision, transformer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://umi1100.github.io/news/diffusion_model/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Umi's Corner </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/news/">Inspirations <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Diffusion Model - A Math-based Simplified Understanding</h1> <p class="post-meta"> March 24, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </p> </header> <article class="post-content"> <div id="markdown-content"> <p>It took me quite some time to comprehend the denoising process of DDPM (Denoising Diffusion Probabilistic Model), one of the most basic variant of diffusion models. In this post, I will elaborate the math behind deriving the objective of DDPM training. I think this math understanding will faciliate the adaptation of other DM variants.</p> <p>As usual, before delving into DDPM, I would like to recommend amazing knowledge sources that helped deepen my understanding.</p> <ul> <li><a href="https://arxiv.org/pdf/1503.03585" rel="external nofollow noopener" target="_blank">Original Diffusion model</a></li> <li><a href="https://arxiv.org/pdf/2006.11239" rel="external nofollow noopener" target="_blank">DDPM paper</a></li> <li><a href="https://arxiv.org/pdf/2209.00796" rel="external nofollow noopener" target="_blank">Survey of Diffusion models</a></li> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">Lilian Weng’s blog post about diffusion model</a></li> <li><a href="https://www.youtube.com/watch?v=687zEGODmHA&amp;t=11s" rel="external nofollow noopener" target="_blank">Berkeley’s lecture video about diffusion model</a></li> </ul> <hr> <h1 id="diffusion-model---ddpm">Diffusion Model - DDPM</h1> <p>Diffusion Models (DM) tries to match the training data distribution by learning to reverse/denoise the diffusion process. Here, diffussion process is the one that gradually destructs the data structure by noise. My focus in this post is on DDPM, the most basic variant of DM that leverages Markov Chain (MC) for both the diffusion and reverse process.</p> <h2 id="discrete-mc-diffusion-process">Discrete MC Diffusion Process</h2> <p>In the diffusion (also called forward) processs, the training data \(x\) is pertubed by a fixed MC through T discrete time steps. At time step 0, let \(x_0\) be an original training sample following distribution (\(q_{data}\)). Then at an arbitrary time step \(t\), \(x_t\) is \(x_{t-1}\) plus a small Gaussian noise characterized by variance \(\beta_t\) such that</p> \[\begin{aligned} &amp; t=0, &amp; x_0 &amp; \sim q_{data}(x) \\ &amp; t = 1:T, &amp; q(x_t|x_{t-1})&amp; \sim N(x_t, \sqrt{1-\beta_t}x_{t-1}, \beta_tI) (1*)\\ &amp; t=T &amp; x_T &amp; \sim N(0, I) (2*) \end{aligned}\] <blockquote> <ul> <li>(1*): the conditional probability of \(x_t\) given \(x_{t-1}\) (\(q(x_t\|x_{t-1})\)) follows a Gaussian distribution with mean \(\sqrt{1-\beta_t}x_{t-1}\) and variance \(\beta_tI\) (\(I\) is an identity matrix), from which we can sample \(x_t\) given \(x_{t-1}\).</li> <li>(2*): T should be sufficiently large so that the final generated sample x_T of the diffusion process is close to Gaussian noise with mean of 0 and unit variance (denoted by \(I\) because the data is usually high-dimensional).</li> </ul> </blockquote> <p>After T time steps, for each sample \(x_0\), we have a sequence of T noisy samples \(x_1, x_2,...,x_T\). The joint distribution conditioned on x_0 is \(q(x_{1:T}\|x_0) = \prod_{t=1}^{T}q(x_t\|x_{t-1})\)</p> <p>This sampling process is stochastic, we can’t backprograte the gradient for model training. Applying the reparameterization trick as in VAE, we have:</p> \[\begin{aligned} q(x_t|x_{t-1}) &amp; = \sqrt{1-\beta_t}x_{t-1} + \sqrt\beta_t\epsilon_{t-1}; \; \text{where}\: \epsilon_{t-1}, \epsilon_{t-2},... \sim N(0,I) \\ &amp; = \sqrt\alpha_tx_{t-1} + \sqrt{1- \alpha_t}\epsilon_{t-1}; \; \text{where}\: \alpha_t = \beta_t - 1 \\ &amp; = \sqrt\alpha_t(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\epsilon_{t-2}) + \sqrt{1- \alpha_t}\epsilon_{t-1}; \\ &amp; \text{where} \: x_{t-1} \: \text{is computed recurvely by} \; x_{t-2} \\ &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon_{t-2} + \sqrt{1- \alpha_t}\epsilon_{t-1}\\ &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t(1-\alpha_{t-1}) + 1-\alpha_t}\bar\epsilon_{t-2}; \; (3*)\\ &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1} }\bar\epsilon_{t-2}\\ &amp; = \sqrt{\alpha_t\alpha_{t-1}\alpha_{t-2}}x_{t-3} + \sqrt{1-\alpha_t\alpha_{t-1}\alpha_{t-2} }\bar\epsilon_{t-3}\\ &amp; = \sqrt{\alpha_t\alpha_{t-1}\alpha_{t-2}...\alpha_1}x_{0} + \sqrt{1-\alpha_t\alpha_{t-1}\alpha_{t-2}...\alpha_{1} }\epsilon\\ &amp; = \cdots\\ &amp; = \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon; \; (4*) \end{aligned}\] <blockquote> <ul> <li>(3*) Combining two Gaussian distributions: \(\epsilon_{t-2} \sim N(0, \alpha_t(1-\alpha_{t-1})I)\) and \(\epsilon_{t-1} \sim N(0, (1-\alpha_t)I)\). The merged distribution is also Gaussian with mean of 0 and variance of the sum of two individual variances: \(\bar \epsilon_{t-2} \sim N(0, (1-\alpha_t + \alpha_t-\alpha_t\alpha_{t-1})I\)</li> </ul> </blockquote> <p>From Eq.(4*), we can sample any (\(x_t\)) directly from \(x_0\): \(q(x_t|x_0) \sim N(x_t; \sqrt{\bar\alpha_t}x_0, \sqrt{1-\bar\alpha_t}I)\)</p> <h2 id="reversedenoising-diffusion-process">Reverse/Denoising Diffusion Process</h2> <p>In the reverse process, DDPM tries to recover \(x_0\) from \(x_T\) by approximating the posterior probability \(q(x_{t-1}\|x_{t}\) to reverse the diffusion process \(q(x_t\|x_{t-1})\). However, it’s very challenging to approximate \(q(x_{t-1}\|x_{t}\) directly because we need the entire training dataset. We only know that \(q(x_{t-1}\|x_{t}\) also follows Gaussion distribution because the noise added at each time step t \(\beta_t\) is small.</p> <p>DDPM instead trains a model (parameterized by \(p_\theta\)) to estimate \(q(x_{t-1}\|x_t)\), says \(p_\theta(x_{t-1}\|x_t)\).</p> \[\begin{aligned} p_\theta(x_{0..T}) = p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}|x_t) \end{aligned}\] <p>Because \(q(x_{t-1}\|x_t)\) follows Gaussian, so does \(p_\theta(x_{t-1}\|x_t)\). Therefore, we can train the model \(p_\theta\) to estimate the mean and variance of the Gaussian \(p_\theta(x_{t-1}\|x_t) \sim N(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_\theta(x_t, t))\)</p> <p>This setup is similar to VAE, where we want to reconstruct the data \(x_0\) from a Gaussian laten variable \(z\). In DDPM, we reconstruct x_0 from a a set of latents or noisy samples \({x_{1..T}}\) and we ccan utilize ELBO to maximize the log-likelihood of the data \(p_\theta(x_0)\).</p> \[\begin{aligned} p_\theta(x_0) &amp; = \int p_\theta(x_{0:T})dx_{1:T}, \text(1*) \\ &amp; = \int p_\theta(x_{0:T}) \frac{q(x_{1:T}|x_0)}{q(x_{1:T}|x_0)}dx_{1:T}, \\ &amp; = E_{x\sim q(x_{1:T}|x_0)}\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)} \\ \end{aligned}\] <p>Apply the log operator to both sides of the equation</p> \[\begin{aligned} -log(p_\theta(x)) &amp; = -log(E_{x\sim q(x_{1:T}|x_0)}\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)} \\ &amp; \le - E_q(log(\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)})) \\ &amp; = E_q(log(\frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})})) = L_{LVB} \end{aligned}\] \[\begin{aligned} L_{LVB} &amp; = E_q(log(\frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})})) \\ &amp; = E_q[log\frac{\prod_{t=1}^T{q(x_{t}|x_{t-1})}}{p_\theta(x_T)\prod_{t=1}^T{p_\theta(x_{t-1}|x_{t})}}] \\ &amp; = E_q[-log(p_\theta(x_T) + \sum_{t=1}^Tlog(\frac{q(x_{t}|x_{t-1})}{p_\theta(x_{t-1}|x_{t})})]\\ &amp; = E_q[-log(p_\theta(x_T) + \sum_{t=2}^Tlog(\frac{q(x_{t}|x_{t-1})}{p_\theta(x_{t-1}|x_{t})}) + log(\frac{q(x_{1}|x_{0})}{p_\theta(x_{0}|x_{1})})] \;\; (5*)\\ &amp; = E_q[-log(p_\theta(x_T) + \sum_{t=2}^Tlog(\frac{q(x_{t-1}|x_{t}, x_0)}{p_\theta(x_{t-1}|x_{t})}) + \sum_{t=2}^Tlog(\frac{q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})}) + log(\frac{q(x_{1}|x_{0})}{p_\theta(x_{0}|x_{1})})] \;\; (6*) \\ &amp; = E_q[-log(p_\theta(x_T) + \sum_{t=2}^Tlog(\frac{q(x_{t-1}|x_{t}, x_0)}{p_\theta(x_{t-1}|x_{t})}) + log(\frac{q(x_{T}|x_{0})}{q(x_{1}|x_{0})}) + log(\frac{q(x_{1}|x_{0})}{p_\theta(x_{0}|x_{1})})] \\ &amp; = E_q[-log(p_\theta(x_T) + \sum_{t=2}^Tlog(\frac{q(x_{t-1}|x_{t}, x_0)}{p_\theta(x_{t-1}|x_{t})}) + log(\frac{q(x_{T}|x_{0})}{p_\theta(x_{0}|x_{1})})] \\ &amp; = E_q[log(\frac{q(x_T|x_0)}{p_\theta(x_T)}) +\sum_{t=2}^Tlog(\frac{q(x_{t-1}|x_{t}, x_0)}{p_\theta(x_{t-1}|x_{t})}) - log(p_\theta(x_0|x_1))] \\ &amp; = E_q[D_{KL}(q(x_T|x_0)||p_\theta(x_T))+ \sum_{t=2}^TD_{KL}(q(x_{t-1}|x_t, x_0)||p_\theta(x_{t-1}|x_t)) - log(p_\theta(x_0|x_1))] \end{aligned}\] <p>Let us denote:</p> \[\begin{aligned} &amp; L_T = D_{KL}(q(x_T|x_0)||p_\theta(x_T))\\ &amp; L_{t} = D_{KL}(q(x_{t}|x_{t+1}, x_0)||p_\theta(x_{t}|x_{t+1}))\; \text{t=1,...,T-1}\\ &amp; L_0 = - log(p_\theta(x_0|x_1)) \end{aligned}\] <p>According to the forward process, \(x_T\) is a Gaussian noise and can be computed directly from \(x_0\). Hence, \(q(x_T\|x_0)\) does not have trainable parameters and can be ignored. \(L_0\) can be optimized by a decoder with input \(x_1\) and output \(x_0\). We only need to focus on \(L_t\)</p> <p>\(L_t\) compares two Guassions \(p_\theta\) and \(q\). It’s notable that \(q(x_t\|x_{t+1}, x_0)\) is tractable and we can derive the Gaussion function for \(q(x_t\|x_{t+1}, x_0) \sim N(x_t; \tilde \mu(x_{t+1}, x_0), \tilde \beta_{t+1}I)\), from which we can modify L_t for a more stable training. Let us delve into this derivation.</p> <p>According to Bayes’s rule,</p> <p>\(\begin{aligned} q(x_{t+1}, x_{t}, x_0) &amp; = q(x_{t+1}|x_t, x_0)q(x_{t}|x_0)q(x_0)\\ &amp; = q(x_{t}|x_{t+1}, x_0)q(x_{t+1}|x_0)q(x_0) \\ q(x_t|x_{t+1}, x_0) &amp; = q(x_{t+1}|x_t, x_0)\frac{q(x_{t}|x_0)}{q(x_{t+1}|x_0)}\\ &amp; \propto \exp(-\frac{1}{2}(\frac{(x_{t+1}-\sqrt{\alpha_{t+1}}x_t)^2}{\beta_{t+1}}+\frac{(x_t-\sqrt{\bar\alpha_t}x_0)^2}{1-\bar\alpha_t} - \frac{(x_{t+1}-\sqrt{\bar\alpha_{t+1}}x_0)^2}{1-\bar\alpha_{t+1}} )) \\ &amp; \propto \exp(-\frac{1}{2}(\frac{(x_{t+1}^2-2\sqrt{\alpha_{t+1}}x_tx_{t+1} + \alpha_{t+1}x_t^2)}{\beta_{t+1}}+\frac{(x_t^2-2\sqrt{\bar\alpha_t}x_0x_t + \bar\alpha_tx_0^2)}{1-\bar\alpha_t} - \frac{(x_{t+1}-\sqrt{\bar\alpha_{t+1}}x_0)^2}{1-\bar\alpha_{t+1}} )) \\ \end{aligned}\) Now we group terms with \(x_t^2\) and \(x_t\) and combine other terms to a constant function C. We call \(C\) a constant function because it’s not dependent on \(x_t\)</p> \[\begin{aligned} q(x_t|x_{t+1}, x_0) &amp; \propto \exp[-\frac{1}{2}((\frac{\alpha_{t+1}}{\beta_{t+1}}+\frac{1}{1-\bar\alpha_t})x_t^2 -2(\frac{\sqrt{\alpha_{t+1}}x_{t+1}}{\beta_{t+1}}+\frac{\sqrt{\bar\alpha_t}x_0}{1-\bar\alpha_t})x_t ) + C(x_{t+1}, x_0)] \\ \end{aligned}\] <p>Now let’s looking at the standard Gaussian density function \(f(x) \propto \exp[-\frac{1}{2}(\frac{x^2}{\sigma^2}-\frac{2\mu x}{\sigma^2}+\frac{\mu^2}{\sigma^2})]\)</p> <p>Recall \(\alpha_t = 1-\beta_t\) and \(\bar\alpha_t = \alpha_1\alpha_2\dots\alpha_t\). We can parameterize \(\begin{aligned} \tilde \beta_{t+1} &amp; = 1/(\frac{\alpha_{t+1}}{\beta_{t+1}}+\frac{1}{1-\bar\alpha_t})\\ &amp; = 1/(\frac{\alpha_{t+1} - \alpha_{t+1}\bar\alpha_t + \beta_{t+1}}{\beta_{t+1}(1-\bar\alpha_t)}) \\ &amp; = \frac{1 - \bar\alpha_{t}}{1-\bar\alpha_{t+1}}\beta_{t+1}\\ \tilde\mu(x_{t+1}, x_0) &amp; = (\frac{\sqrt{\alpha_{t+1}}x_{t+1}}{\beta_{t+1}}+\frac{\sqrt{\bar\alpha_t}x_0}{1-\bar\alpha_t})/(\frac{\alpha_{t+1}}{\beta_{t+1}}+\frac{1}{1-\bar\alpha_t}) \\ &amp; = (\frac{\sqrt{\alpha_{t+1}}x_{t+1}}{\beta_{t+1}}+\frac{\sqrt{\bar\alpha_t}x_0}{1-\bar\alpha_t})\frac{1 - \bar\alpha_{t}}{1-\bar\alpha_{t+1}}\beta_{t+1} \\ &amp;= \frac{\sqrt{\alpha_{t+1}}(1-\bar\alpha_t)}{1-\bar\alpha_{t+1}}x_{t+1} + \frac{\sqrt{\bar\alpha_t}\beta_{t+1}}{(1-\bar\alpha_{t+1})}x_0 \end{aligned}\)</p> <p>From the forward process, we have \(x_0 = \frac{1}{\sqrt{\bar\alpha_{t+1}}}(x_{t+1} - \sqrt{1-\bar\alpha_{t+1}}\epsilon_{t+1})\)</p> \[\begin{aligned} \tilde\mu(x_{t+1}, x_0) &amp; = \frac{\sqrt{\alpha_{t+1}}(1-\bar\alpha_t)}{1-\bar\alpha_{t+1}}x_{t+1} + \frac{\sqrt{\bar\alpha_t}\beta_{t+1}}{(1-\bar\alpha_{t+1})}(\frac{1}{\sqrt{\bar\alpha_{t+1}}}(x_{t+1} - \sqrt{1-\bar\alpha_{t+1}}\epsilon_{t+1}))\\ &amp; = \frac{\sqrt{\alpha_{t+1}}(1-\bar\alpha_t)}{1-\bar\alpha_{t+1}}x_{t+1} + \frac{\beta_{t+1}}{(1-\bar\alpha_{t+1})\sqrt{\alpha_{t+1}}}(x_{t+1}-\sqrt{1-\bar\alpha_{t+1}}\epsilon_{t+1})\\ &amp; = \frac{\alpha_{t+1}(1-\bar\alpha_t) + \beta_{t+1}}{(1-\bar\alpha_{t+1})\sqrt{\alpha_{t+1}}}x_{t+1} - \frac{1- \alpha_{t+1}}{\sqrt{1-\bar\alpha_{t+1}}\sqrt{\alpha_{t+1}}}\epsilon_{t+1}\\ &amp; = \frac{1-\bar\alpha_{t+1}}{(1-\bar\alpha_{t+1})\sqrt{\alpha_{t+1}}}x_{t+1} - \frac{1- \alpha_{t+1}}{\sqrt{1-\bar\alpha_{t+1}}\sqrt{\alpha_{t+1}}}\epsilon_{t+1}\\ &amp; = \frac{1}{\sqrt{\alpha_{t+1}}}(x_{t+1}- \frac{1-\alpha_{t+1}}{\sqrt{1-\bar\alpha_{t+1}}}\epsilon_{t+1}) \end{aligned}\] <p>Recall we want to train a neural network to learn \(p_\theta(x_t|x_{t+1})\) and \(p_\theta(x_t|x_{t+1}) \sim N(x_t; \mu_\theta(x_{t+1}, t), \Sigma_\theta(x_{t+1},t))\). To train the network, we need to minimize the KL divergence between \(q(x_t|x_{t+1}, x_0)\) and \(p_\theta(x_{t}|x_{t+1})\). From the density function of \(q(x_t|x_{t+1}, x_0)\) derived above, we can try to predict \(\tilde\mu = \frac{1}{\sqrt{\alpha_{t+1}}}(x_{t+1}- \frac{1-\alpha_{t+1}}{\sqrt{1-\bar\alpha_{t+1}}}\epsilon_{t+1})\). Because \(x_{t+1}\) is provided as input in the reverse process, we can parameterize the network to predict \(\epsilon_{t+1}\) and \(p_\theta(x_t|x_{t+1}) \sim N(x_t; \frac{1}{\sqrt{\alpha_{t+1}}}(x_{t+1}- \frac{1-\alpha_{t+1}}{\sqrt{1-\bar\alpha_{t+1}}}\epsilon_{\theta}(x_{t+1}, t)), \Sigma_\theta(x_{t+1},t))\)</p> <p>The KL divergence between \(q(x_t|x_{t+1}, x_0)\) and \(p_\theta(x_{t}|x_{t+1})\) is the difference between two mean \(\tilde\mu\) and \(\mu_\theta\)</p> \[\begin{aligned} L_t &amp;= D_{KL}(q(x_t|x_{t+1}, x_0)|p_\theta(x_{t}|x_{t+1})) \\ &amp; = E_{x_0, \epsilon}(\frac{1}{2\|\Sigma_\theta(x_{t+1}, t)\|_2^2}\|(\tilde\mu_t(x_{t+1}, x_0)-\mu_\theta(x_{t+1}, t))\|^2\\ &amp;=E_{x_0, \epsilon}(\frac{1}{2\|\Sigma_\theta\|_2^2}\|(\frac{1}{\sqrt{\alpha_{t+1}}}(x_{t+1}- \frac{1-\alpha_{t+1}}{\sqrt{1-\bar\alpha_{t+1}}}\epsilon_{t+1})-\frac{1}{\sqrt{\alpha_{t+1}}}(x_{t+1}- \frac{1-\alpha_{t+1}}{\sqrt{1-\bar\alpha_{t+1}}}\epsilon_{\theta}(x_{t+1}, t))\|^2\\ &amp;=E_{x_0, \epsilon}(\frac{1-\alpha_{t+1}}{2\alpha_{t+1}\|\Sigma_\theta\|_2^2(1-\bar\alpha_{t+1})}\|\epsilon_{t+1}-\epsilon_{\theta}(x_{t+1}, t)\|^2\\ \end{aligned}\] <p>With this \(L_t\), we can train a network to predict the injected noise at each time step \(t\).</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Umi's Corner. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>